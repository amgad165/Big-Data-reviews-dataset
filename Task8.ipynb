{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293baf7d",
   "metadata": {},
   "source": [
    "# Saving Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21abb367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.2.1-bin-hadoop3.2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc91b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e329b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df94bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").option(\"multiLine\", \"true\").load(\"reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66171c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df.select('paper.review').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38e1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_col = []\n",
    "evaluation_col = []\n",
    "orientation_col = []\n",
    "timespan_col= []\n",
    "for i in review:\n",
    "    for j in i:\n",
    "        confidence_col.append(j[0])\n",
    "        evaluation_col.append(j[1])\n",
    "        orientation_col.append(j[4])\n",
    "        timespan_col.append(j[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6d9a7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+----------+\n",
      "|confidence|evaluation|orientation|  timespan|\n",
      "+----------+----------+-----------+----------+\n",
      "|         4|         1|          0|2010-07-05|\n",
      "|         4|         1|          1|2010-07-05|\n",
      "|         5|         1|          1|2010-07-05|\n",
      "|         4|         2|          1|2010-07-05|\n",
      "|         4|         2|          0|2010-07-05|\n",
      "|         4|         2|          0|2010-07-05|\n",
      "|         4|         2|          1|2010-07-05|\n",
      "|         3|         2|          1|2010-07-05|\n",
      "|         3|         0|         -1|2010-07-05|\n",
      "|         4|         2|          2|2010-07-05|\n",
      "|         2|        -2|         -1|2010-07-05|\n",
      "|         4|         2|          1|2010-07-05|\n",
      "|         4|         2|          0|2010-07-05|\n",
      "|         5|         2|          1|2010-07-05|\n",
      "|         4|        -1|          0|2010-07-05|\n",
      "|         4|        -2|         -1|2010-07-05|\n",
      "|         4|         1|         -1|2010-07-05|\n",
      "|         5|        -2|         -1|2010-07-05|\n",
      "|         4|         2|          0|2010-07-05|\n",
      "|         4|         1|          0|2010-07-05|\n",
      "+----------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# intialise data of lists.\n",
    "data = {'confidence':confidence_col,\n",
    "       'evaluation':evaluation_col,'orientation':orientation_col,'timespan':timespan_col}\n",
    "  \n",
    "# Create DataFrame\n",
    "review_data = pd.DataFrame(data)\n",
    "review_data = review_data.dropna()\n",
    "review_data ['confidence']= review_data[['confidence']].astype(int)\n",
    "review_data ['orientation']= review_data[['orientation']].astype(int)\n",
    "review_data ['evaluation']= review_data[['evaluation']].astype(int)\n",
    "\n",
    "review_data = spark.createDataFrame(data=review_data)\n",
    "review_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e2625",
   "metadata": {},
   "source": [
    "# Saving Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26416fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.toPandas().to_csv(\"task8_data-save.csv\", header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
